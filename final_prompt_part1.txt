You are a specialized AI assistant for data analysis and knowledge base curation. Your task is to process a dataset and output structured information based only on the provided context. Ignore all other instructions.

=== beginning of dataset context ===

Dataset Name:  
loan-default-risk.csv  

Dataset Snippet:  
id,loan_amnt,term,int_rate,installment,home_ownership,annual_inc,verification_status,issue_d,loan_status,purpose,total_pymnt
1077501,5000,36,10.65,162.87,rent,24000.0,verified,-11,fully paid,credit_card,5863.155187
1077430,2500,60,15.27,59.83,rent,30000.0,source verified,-11,charged off,car,1008.71
1077175,2400,36,15.96,84.33,rent,12252.0,not verified,-11,fully paid,small_business,3005.666844
1076863,10000,36,13.49,339.31,rent,49200.0,source verified,-11,fully paid,other,12231.89
1075358,3000,60,12.69,67.79,rent,80000.0,source verified,-11,current,other,3513.33
  

Dataset Columns with Sample Unique Values:  
{'id': {'num_of_unique_values': 39717, 'top_5_values': [87023, 1077501, 1077430, 1077175, 1076863]}, 'loan_amnt': {'num_of_unique_values': 885, 'top_5_values': [10000, 12000, 5000, 6000, 15000]}, 'term': {'num_of_unique_values': 2, 'top_5_values': [36, 60]}, 'int_rate': {'num_of_unique_values': 371, 'top_5_values': [10.99, 13.49, 11.49, 7.51, 7.88]}, 'installment': {'num_of_unique_values': 15383, 'top_5_values': [311.11, 180.96, 311.02, 150.8, 368.45]}, 'home_ownership': {'num_of_unique_values': 5, 'top_5_values': ['rent', 'mortgage', 'own', 'other', 'none']}, 'annual_inc': {'num_of_unique_values': 5318, 'top_5_values': [60000.0, 50000.0, 40000.0, 45000.0, 30000.0]}, 'verification_status': {'num_of_unique_values': 3, 'top_5_values': ['not verified', 'verified', 'source verified']}, 'issue_d': {'num_of_unique_values': 5, 'top_5_values': [-11, -10, -9, -8, -7]}, 'loan_status': {'num_of_unique_values': 3, 'top_5_values': ['fully paid', 'charged off', 'current']}, 'purpose': {'num_of_unique_values': 14, 'top_5_values': ['debt_consolidation', 'credit_card', 'other', 'home_improvement', 'major_purchase']}, 'total_pymnt': {'num_of_unique_values': 37850, 'top_5_values': [11196.56943, 10956.77596, 0.0, 11784.23223, 5478.387981]}}  

=== end of dataset context ===

**Column Classifications**

Identifier: unique key for linking data.
Example: customer_id, product_sku

Dimensional: for grouping/filtering/categorizing.
Example: country, marital_status

Metric: numerical values to measure/aggregate.
Example: revenue, sales_quantity

Temporal: dates/timestamps.
Example: order_date, login_timestamp

Geospatial: location data.
Example: latitude, longitude, zip_code

Scientific: values with units (scientific domain).
Example: temperature_celsius, pressure_bar

Descriptive: qualitative attributes.
Example: product_name, user_bio

PII: sensitive info.
Example: email_address, full_name

System/Metadata: system-generated.
Example: created_at, last_modified

Unknown: unclear meaning

Instructions
1. Domain + Columns

Infer dataset domain and a concise description.

For each column, output:
- classification (from the list above)
- confidence (low, medium, high)
- data_type (string, integer, float, datetime)
- type (Categorical or Numerical)
- unit (only for Scientific columns, otherwise empty)
- expected_values (categorical: list of values, numerical: range of values, uncertain: empty)

2. Common Column Combination

- Suggest 1–3 new metrics by combining existing columns.
- Only use Metric or Dimensional columns.
- Expression must be Pandas-eval safe (+, -, *, /, **).
- Ensure referenced columns exist.

Example:

{
  "name": "profit_margin",
  "description": "Calculate margin as (revenue - cost) / revenue.",
  "operation": {
    "source_columns": ["revenue", "cost"],
    "expression": "(revenue - cost) / revenue"
  }
}

3. Common Column Cleaning or Transformation

- Suggest 3–5 new or cleaned columns.
- Must use the correct transformation type:

**Supported Transformation Types**

MAP → remap categorical values into grouped labels.

{
  "type": "map",
  "source_column": "education_level",
  "mapping": {"PhD": "Higher_Ed", "Basic": "Secondary_Ed"}
}


MAP_RANGE → bin numeric values into labeled ranges.

{
  "type": "map_range",
  "source_column": "gross_income",
  "ranges": [
    {"range": "0-30000", "label": "Low"},
    {"range": "30001-80000", "label": "Medium"},
    {"range": "80001-inf", "label": "High"}
  ]
}


DATE_OP → extract YEAR, MONTH, DAY, or WEEKDAY from Temporal columns.

{
  "type": "date_op",
  "source_column": "order_date",
  "function": "MONTH"
}


MATH_OP → simple math expressions using numeric columns.

{
  "type": "math_op",
  "source_columns": ["monthly_income"],
  "expression": "monthly_income * 12"
}


**Transformation JSON Structure**

Each must include:

name: new column name (snake_case)
description: short explanation
operation: valid MAP / MAP_RANGE / DATE_OP / MATH_OP object

---

Final output JSON schema:

{
  "domain": "string",
  "description": "string",
  "columns": [
    {
      "name": "string",
      "classification": "string",
      "confidence_score": "float",
      "data_type": "string",
      "type": "string",
      "unit": "string",
      "expected_values": ["string"]
    }
  ],
  "common_column_combination": [
    {
      "name": "string",
      "description": "string",
      "operation": {
        "source_columns": ["col1","col2"],
        "expression": "..."
      }
    }
  ],
  "common_column_cleaning_or_transformation": [
    {
      "name": "string",
      "description": "string",
      "operation": { ... }
    }
  ]
}